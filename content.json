{"meta":{"title":"熊猫菌的小站","subtitle":"加油，打工人！","description":"站在巨人的肩膀上！","author":"熊猫菌","url":"https://github.com/moexiong/moexiong.github.io/tree/master","root":"/"},"pages":[{"title":"","date":"2021-05-09T13:29:07.970Z","updated":"2021-05-09T13:29:07.970Z","comments":true,"path":"data/sentences.json","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/data/sentences.json","excerpt":"","text":"[{\"content\":\"错的不是我，是世界。\",\"author\":\"鲁路修·兰佩路基\",\"from\":\"Code Geass 反叛的鲁路修\"},{\"content\":\"我们一日日度过的所谓日常，实际上可能是接连不断的奇迹。\",\"from\":\"日常\"},{\"content\":\"隐约雷鸣 阴霾天空 但盼风雨来 能留你在此。\",\"from\":\"万叶集·雷神短歌\"},{\"content\":\"人类的悲欢并不相通，我只觉得他们吵闹。\",\"author\":\"鲁迅\",\"from\":\"小杂感\"},{\"content\":\"遍身罗绮者，不是养蚕人。\",\"author\":\"张俞\",\"from\":\"蚕妇\"},{\"content\":\"今日は……风が騒がしいな…（今日的风儿甚是喧嚣……）\",\"author\":\"田畑秀则\",\"from\":\"男子高中生的日常\"},{\"content\":\"人类的赞歌是勇气的赞歌！人类的伟大是勇气的伟大！！\",\"author\":\"威廉·A·齐贝林\",\"from\":\"JOJO 的奇妙冒险 幻影之血\"},{\"content\":\"人生就像蒲公英，看似自由，实则身不由己。\",\"from\":\"日常\"},{\"content\":\"大师，什么是快乐的秘诀？「不要和愚者争论。」大师，我完全不同意这就是秘诀。「是的，你是对的。」\",\"from\":\"日常\"}]"},{"title":"异世界","date":"2021-05-09T14:42:00.107Z","updated":"2021-05-09T14:42:00.107Z","comments":true,"path":"/404.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/404.html","excerpt":"","text":""},{"title":"没错，我就是二刺螈","date":"2021-05-09T14:13:42.000Z","updated":"2021-05-10T15:12:13.035Z","comments":true,"path":"about/index.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/about/index.html","excerpt":"","text":"不积跬步无以至千里，不积小流无以成江河！长路慢慢修远兮，每天都要笑嘻嘻！"},{"title":"文章分类","date":"2021-05-09T14:12:21.000Z","updated":"2021-05-09T14:39:06.027Z","comments":false,"path":"categories/index.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/index.html","excerpt":"","text":""},{"title":"膜拜的大佬们","date":"2021-05-12T14:36:14.107Z","updated":"2021-05-12T14:36:14.107Z","comments":true,"path":"links/index.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/links/index.html","excerpt":"","text":""},{"title":"文章标签","date":"2021-05-09T14:10:38.000Z","updated":"2021-05-09T14:39:09.708Z","comments":false,"path":"tags/index.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/index.html","excerpt":"","text":""},{"title":"可爱的女孩子","date":"2021-05-09T14:27:40.522Z","updated":"2021-05-09T14:27:40.522Z","comments":true,"path":"girls/index.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/girls/index.html","excerpt":"","text":""},{"title":"关于站点","date":"2021-05-09T14:13:42.000Z","updated":"2021-05-09T14:53:44.690Z","comments":true,"path":"about/site.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/about/site.html","excerpt":"","text":"好像没啥!"},{"title":"收藏的资源","date":"2021-05-24T13:43:00.285Z","updated":"2021-05-24T13:43:00.285Z","comments":true,"path":"stores/index.html","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/stores/index.html","excerpt":"","text":""}],"posts":[{"title":"Redis 基础知识","slug":"Redis 基础知识","date":"2021-05-25T15:45:56.556Z","updated":"2021-05-25T15:51:49.482Z","comments":true,"path":"2021/05/25/Redis 基础知识/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/25/Redis%20%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"Redis 基础知识Redis 介绍(百度)redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。Redis 是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。这使得Redis可执行单层树复制。存盘可以有意无意的对数据进行写操作。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助。 Redis 对象Redis的key只是字符串类型，但是value却支持多种对象类型，server.h中redisObject的结构定义为： 123456789typedef struct redisObject &#123; unsigned type:4; // 4bits，对象类型 unsigned encoding:4; // 4bits，对象存储格式 unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits access time).(3bytes) */ int refcount; // 32bits(4bytes)，对象引用计数，为0时被回收。 void *ptr; // 对象地址指针。8bytes=64位系统。不考虑32位系统。&#125; robj; 一个 RedisObject 对象头共需要占据 16 字节的存储空间。注：均基于Redis6.2.3版本源码分析。 数据类型 type取值 说明 OBJ_STRING 0 字符串对象 (实际可以为整型，浮点型以及字符串) OBJ_LIST 1 列表对象（实际队列，元素可以重复，支持FIFO原则，对比Java的List） OBJ_SET 2 set (不可重复的集合，对比Java的Set) OBJ_ZSET 3 sorted set (有序集合) OBJ_HASH 4 hash (Hash表) OBJ_MODULE 5 模块对象（在RDB文件中的编码形式为，具有64位的模块ID，其中54位为模块特定签名，10位为编码版本） OBJ_STREAM 6 流对象（5.0版本以后新增的，为了更好的当做消息队列使用） 编码类型 encoding取值 说明 Object encoding命令输出 OBJ_ENCODING_RAW 0 简单动态字符串 raw OBJ_ENCODING_INT 1 long类型的整数 int OBJ_ENCODING_HT 2 字典 hashtable OBJ_ENCODING_ZIPMAP 3 压缩字典 无了 OBJ_ENCODING_LINKEDLIST 4 双端链表 无了 OBJ_ENCODING_ZIPLIST 5 压缩列表 ziplist OBJ_ENCODING_INTSET 6 整数集合 intset OBJ_ENCODING_SKIPLIST 7 跳跃表 skiplist OBJ_ENCODING_EMBSTR 8 embstr编码的简单动态字符串 embstr OBJ_ENCODING_QUICKLIST 9 快速列表 quicklist OBJ_ENCODING_STREAM 10 流 stream 字符串对象（String）字符串对象支持的存储格式有3种： OBJ_ENCODING_INT：当存储的是数值类型的时。 OBJ_ENCODING_EMBSTR：当存储的是字符串且长度小于等于44字节时。 OBJ_ENCODING_RAW：当存储的是字符串且长度大于44字节时。 123456struct __attribute__ ((__packed__)) sdshdr[5|8|16|32|64] &#123; uint[8|16|32|64]_t len; // 已使用的长度 (为5时没有) uint[8|16|32|64]_t alloc; // 分配的总长度 (为5时没有) unsigned char flags; // 3bits标识，5bits没有用。5=000，8=001，16=010，32=011,64=100。 char buf[];&#125;; SDSHDR的结构大致如下： flags=0的场景有待验证，从源码中看是定长，与C字符串类似，可能用作常量？ 横向表示为内存地址空间。 OBJ_ENCODING_INT如果要存储一个数值为100的字符串进去INT示意图： 数值类型的值，直接存储在*ptr指针所指向的内存，没有额外的空间引用，不需要使用SDSHDR结构。 OBJ_ENCODING_EMBSTR如果要存储一个Hello进去(小于44字节)。 123#define OBJ_ENCODING_EMBSTR_SIZE_LIMIT 44 // 定义为44bytes长度。// 计算公式如下：robj=16bytes,sdshdr8=3bytes,char[]=44bytes,补齐1bytes。robj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1); EMBSTR示意图： 当字符串长度小于等于44字节时，会以EMBSTR编码的SDSHDR类型存储。 SDSHDR的内存分配是与REDIS_OBJECT一起申请的，所以它们是连续的空间内存，一次内存分配即可完成。 EMBSTR类型一般不修改，如果要修改，要么长度仍小于44字节重建，要么长度大于44字节类型变换为RAW。 OBJ_ENCODING_RAW如果存储一个大于44bytes的字符串abcd...(省略)。RAW字符串示意： 当字符串长度大于44字节时，会以RAW编码的SDSHDR类型存储。 SDSHDR的内存分配是单独的一次，这是和EMBSTR不一样的地方，需要两次内存分配才行，不连续。 预留空间充足时，字符串改变无需重写分配内存，不够时需要重建。 列表对象（List）列表对象可支持的存储格式有2种： OBJ_ENCODING_ZIPLIST：当列表中对象少（小于512个），仅有INT或短字符串(小于64bytes)时，使用压缩列表。 OBJ_ENCODING_QUICKLIST：当列表中的对象多，或字符串类型长，变动频繁时，用快速压缩列表。 OBJ_ENCODING_ZIPLISTZIPLIST的结构定义如下： 12345678910111213typedef struct zlentry &#123; // prevrawlensize prevrawlen通常会被压缩，小于255的用1个字节来表示，大于等于255的用5个字节表示 unsigned int prevrawlensize; // 编码前驱节点的所需的字节大小 unsigned int prevrawlen; // 前驱节点的内容大小 // lensize len通常也会被压缩，与preraw同理。 unsigned int lensize; // 编码当前节点的所需的字节大小 unsigned int len; // 当前节点的内容大小 unsigned int headersize; // prevrawlensize + lensize unsigned char encoding; // 对字符数组或整数的编码方式，ZIP_STR_*,不同位编码长度不一样，1~5字节；ZIP_INT_*，整数编码长度只有1字节 unsigned char *p; // 指向当前节点实际值的指针&#125; zlentry; 压缩列表示意： 压缩列表本是单向链表，但是由于内存空间存储的连续性，使得可以从header向tail遍历，所以可以看做双向链表，弥补了单向链表的查询弊端，但是修改会重新分配内存空间以维护连续性，所以修改的效率不一定是O(1)，列表越大，效率越低。 由于会分配一个节点会记录其前驱的长度，当节点长度小于255字节时，默认只会采用一个bytes去记录节点的前驱长度，为了节省内存，但是当有一个节点大于或等于255字节时，一个bytes不够，需要扩容为5个bytes来记录前驱节点的长度，为了不频繁更新，redis直接进行连锁更新，将后续所有节点的前驱长度记录扩容为5bytes，若后续节点都是250~253bytes，最差情况插入头节点大于等于255，引发全部节点的内存重新分配，删除同理。 redis为了防止发生数组抖动，一会儿扩一会儿缩这种，**不处理因为节点的变小而引发的连锁更新，防止出现反复的缩小-扩展(flapping，抖动)**。 OBJ_ENCODING_QUICKLIST快速列表由压缩列表构成，类似于分段的思想，将其拆分为多组ZIPLIST。快速列表的结构定义如下： 1234567891011121314151617181920212223typedef struct quicklist &#123; quicklistNode *head; // 头节点指针 quicklistNode *tail; // 尾节点指针 unsigned long count; // 所有ZIPLIST中的entry个数，即整个QUICKLIST中的entry个数 unsigned long len; // quicklistNode节点个数，也就是ZIPLIST的个数 int fill : QL_FILL_BITS; // 唯一标识，补齐字节位的，实际没啥用 unsigned int compress : QL_COMP_BITS; // 压缩程度 unsigned int bookmark_count: QL_BM_BITS; quicklistBookmark bookmarks[];&#125; quicklist;typedef struct quicklistNode &#123; struct quicklistNode *prev; // 前驱节点 struct quicklistNode *next; // 后继节点 unsigned char *zl; // 指向当前ZIPLIST的指针 unsigned int sz; // ZIPLIST的字节大小长度 unsigned int count : 16; // ZIPLIST的entry个数 unsigned int encoding : 2; // RAW==1 or LZF==2，编码方式，RAW=原始，LZF=压缩 unsigned int container : 2; // NONE==1 or ZIPLIST==2 unsigned int recompress : 1; // 前驱节点是否压缩？ unsigned int attempted_compress : 1; // 无法压缩标识，即使指定了LZF也压缩不了 unsigned int extra : 10; // 扩展空间&#125; quicklistNode; 快速列表示意： 直接就是一个双向链表，进行插入或删除操作时非常方便，虽然复杂度为O(n)，但是不需要内存的复制，提高了效率，而且访问两端元素复杂度为O(1)。 每个entry继承了ZIPLIST的优点，顺序存储，内存连续，利用二分查找，对于ZIPLIST的查找效率较高。 因为每个ZIPLIST都是小节点，默认不超过8kb，所以发生连锁更新的情况也会比较小。 集合对象（Set）集合对象可支持的存储格式有2种： OBJ_ENCODING_INTSET：当集合中的值全是整数，或者对象数量不超过512个时，采用整数集合(IntSet)。 OBJ_ENCODING_HT：不满足上述任一条件后，采用字典(HashTable)。 OBJ_ENCODING_INTSET整数集合的结构定义如下： 12345typedef struct intset &#123; uint32_t encoding; // 整数的存储编码形式，INTSET_ENC_INT[16|32|64]，决定了编码后的字节大小 uint32_t length; // 集合大小 int8_t contents[]; // 元素数组&#125; intset; 整数集合示意： 采用了数组作为存储结构，有序。使用二分查找快速定位元素。 不同编码会影响集合的空间申请大小，当整数由小编码转大编码时，将数组扩容为大编码格式，需要扩容内存空间，反之则不会。道理同ZIPLIST的扩容，为了防止抖动。 OBJ_ENCODING_HTHT即HashTable，也就是字典。字典的结构定义如下： 1234567891011121314151617181920212223242526272829303132333435typedef struct dict &#123; dictType *type; // 类型特定函数 void *privdata; // 指向私有数据的指针 dictht ht[2]; // 2个Hash表，在进行扩容resize的时候，会用到第2个Hash表，保证读写不受影响。 long rehashidx; // rehash目前的进度，没有发生rehash时为-1 int16_t pauserehash; // 如果大于0表示rehashing暂停，如果小于0表示编码有错误&#125; dict;typedef struct dictType &#123; uint64_t (*hashFunction)(const void *key); // 计算Hash值 void *(*keyDup)(void *privdata, const void *key); // 复制键 void *(*valDup)(void *privdata, const void *obj); // 复制值 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 对比键 void (*keyDestructor)(void *privdata, void *key); // 销毁键 void (*valDestructor)(void *privdata, void *obj); // 销毁值 int (*expandAllowed)(size_t moreMem, double usedRatio); // 是否需要扩展空间&#125; dictType;typedef struct dictht &#123; dictEntry **table; // Hash表数组 unsigned long size; // Hash表大小，数组总长度 unsigned long sizemask; // Hash表长度size-1，用作索引 unsigned long used; // 已使用的数组空间长度&#125; dictht;typedef struct dictEntry &#123; void *key; // 指向key的指针 union &#123; void *val; // 指向value的指针，非数值类型，指向另外的地址空间 uint64_t u64; // 整数编码格式 int64_t s64; // 整数编码格式 double d; // 数值类型直接存这，不需要额外空间 &#125; v; struct dictEntry *next; // 指向下个entry节点&#125; dictEntry; 字典示意： 负载因子 = 哈希表当前保存节点数 / 哈希表大小。当没有执行BGSAVE或BGREWRITEAOF时，负载因子大于等于1时就会发送扩展操作。否则当负载因子大于等于5时才会发生扩展操作。当负载因子小于0.1时会发生收缩操作。 采用2张Hash表，渐进式扩展，不会明显降低当前hash表的访问效率。 有序集合对象（ZSet）有序集合对象可支持的存储格式有2种： OBJ_ENCODING_ZIPLIST：压缩列表，同上所述。 OBJ_ENCODING_SKIPLIST：跳跃表，类似于B*树，底层是一个双向链表，上层单向链表。 OBJ_ENCODING_ZIPLIST与列表对象的ZIPLIST一致。 OBJ_ENCODING_SKIPLIST跳跃表的结构定义如下：6.2.3没找到？ 哈希对象（Hash）哈希对象可支持的存储格式有2种： OBJ_ENCODING_ZIPLIST：压缩列表，同上所述。 OBJ_ENCODING_HT：字典，同上所述。 参考资料Redis 6.2.3源码redis 系列，要懂redis，首先得看懂sds（全网最细节的sds讲解）redis源码分析-intset(整型集合)Redis 压缩列表(ziplist)和快速列表(quicklist)Redis五种数据类型详解Redis数据结构——快速列表(quicklist)Redis数据结构——dict（字典）","categories":[{"name":"缓存中间件","slug":"缓存中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E7%BC%93%E5%AD%98%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Redis/"},{"name":"缓存","slug":"缓存","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E7%BC%93%E5%AD%98/"}],"author":"熊猫菌"},{"title":"Nginx 基础知识","slug":"Nginx 基础知识","date":"2021-05-18T15:10:44.410Z","updated":"2021-05-19T14:46:25.912Z","comments":true,"path":"2021/05/18/Nginx 基础知识/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/18/Nginx%20%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"Nginx 基础知识Nginx介绍Nginx是俄罗斯人Igor Sysoev开发的轻量级Web服务器，不仅仅是一个高性能的HTTPP和反向代理服务器，同时也是一个IMAP/POP3/SMTP服务器。它是以事件驱动的方式编写，所以有非常好的性能，同时也是一个非常高效的反向代理、负载平衡服务器。在性能上，Nginx占用很少的系统资源，能支持更多的并发连接，达到更高的访问效率；在功能上，Nginx是优秀的代理服务器和负载均衡服务器；在安装配置上，Nginx安装简单、配置灵活。Nginx支持热部署，启动速度特别快，还可以在不间断服务的情况下对软件版本或配置进行升级，即使运行数月也无需重新启动。在微服务的体系之下，Nginx正在被越来越多的项目采用作为网关来使用，配合Lua做限流、熔断等控制。 Nginx 优点 跨平台：Nginx可以在大多数UNix like OS平台运行，也有windows的移植版。 配置非常简单：配置风格与程序开发类似，易于理解。 非阻塞，高并发连接：官方测试可以支撑5万并发连接，实际中可以达到2~3万。 事件驱动：如果在linux上，默认采用epoll模型，支持更大的连接数。 热部署能力：一个主进程，多个工作进程，可以直接修改配置生效而无需重启。 内存消耗小：处理大并发的请求内存消耗非常小，在3万并发连接中，10个Nginx才消耗150M内存。 内置的健康检测：Nginx的某台后端服务器宕机，不会影响前端访问。(其他使用KeepAlived也可达到此效果) 节省带宽：支持GZIP压缩，可以添加浏览器本地缓存的请求头。 可用作HTTP正反向代理，负载均衡和动静分离。 Nginx 缺点 Nginx仅能支持Http，Https和SMTP协议，在适用范围上小。 内置的健康检测仅能支持端口，不支持url检测。不用ip_hash负载无法支持对session的保持。(session共享也行) Nginx示意： Master进程负责监控和通知Worker进程处理连接，通过accept_mutex信号量来控制Worker竞争连接。(对比Netty BossGroup和WorkerGroup) 一个请求只能被一个Worker处理，一个Worker只有一个主线程，所以同时也只能处理一个请求。 Nginx 正向代理当客户端无法直接与服务器建立连接时(没错，就是你GFW的锅)。我们就需要一个正向代理来为我们搭建连接建立的桥梁。 正向代理示意： 客户端明确的知道要访问的是代理服务器。真实的目标交由代理服务器去连接。 Nginx 反向代理看起来好像和正向代理的访问模式一样，都是客户端连接Nginx，Nginx连接到服务器。 反向代理示意： 客户端真正期望连接的www.ab.com实际是指服务器A或服务器B。但是实际连接的却是Nginx，真实的目标服务器被隐藏了。 最大并发数：worker_connection * worker_processes / 4。占用2个连接，客户端-worker，worker-服务器。 worker_connection：一个Worker的最大连接数 worker_processes：工作进程数 因为一个请求要占用2个或4个连接数。 Nginx 负载均衡就是对请求进行一个调度，默认的策略是轮询调度，均匀的分配给每个服务器。 upstream模块下负载均衡的配置策略： 轮询(默认)：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动踢除。 weight：指定轮询权重，weight和访问比例成正比，值越大，请求越多。 ip_hash：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 fair(第三方)：按后端服务器的响应时间来进行分配，响应时间短的优先分配。 url_hash(第三方)：按访问url到hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 Nginx 动静分离把一些css，js，图片等脚本静态资源内容放在另一台服务器上，类似于CDN一般，这样就可以把加载时的资源分离开来，实现单独的负载均衡等。 动静分离示意： 不分离的话，一次请求既要包括大量IO操作，同时动态请求还包含了大量CPU计算操作，对于服务器的压力较大。而且静态资源设置了过期时间后，一定时间内的请求都可以无需加载。 Nginx 事件模型Nginx支持5种连接处理方法，可以通过use来指定： select：标准方法。如果当前平台没有更高效的方法，属于编译时默认的方法，可以使用配置参数--with-select_module和--without-select_module poll：标准方法。 如果当前平台没有更有效的方法，它是编译时默认的方法。你可以使用配置参数--with-poll_module和--without-poll_module kqueue：高效的方法，使用于 FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X. 使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 epoll：高效的方法，使用于Linux内核2.6版本及以后的系统。在某些发行版本中，如SuSE 8.2, 有让2.4版本的内核支持epoll的补丁。 /dev/poll：高效的方法，使用于 Solaris 7 11/99+, HP/UX 11.22+ (eventport), IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+. eventpoll：高效的方法，使用于 Solaris 10.为了防止出现内核崩溃的问题，需要安装安全补丁。 Nginx 优化编译安装过程优化编译Nginx的时候，默认以DEBUG模式运行，该模式下会插入很多跟踪与assert之类的信息，编译完成后，一个Nginx有好几M大小，而如果取消DEBUG模式，则可以减少占用空间，大概几百K的大小，修改方式如下：找到源码目录下/auto/cc/gcc文件，找到# debugCFLAGS=&quot;$CFLAGS -g&quot;删除或注释掉这2行即可取消debug模式编译。 为指定CPU进行类型编译优化编译Nginx的时候，默认的GCC编译参数是”-O”，要优化GCC编译，可以使用以下两个参数：--with-cc-opt=&#39;-O3&#39;--with-cpu-opt=CPU：CPU即为特定的CPU编译，有效值包括： pentium pentiumpro pentium3 pentium4 athlon opteron amd64 sparc32 sparc64 ppc64 确认CPU类型：cat /proc/cpuinfo | grep &quot;model name&quot; 利用TCMalloc库TCMalloc的全称为Thread-Caching Malloc，是谷歌开源工具google-perftools中的一个成员，比标准glibc库的内存分配效率和速度上要快很多。提高了服务器高并发下的性能，降低负载。 TCP内核参数可以定制一些TCP内核参数，来让Nginx选择对连接的处理方式。具体配置参数在TCP/IP内介绍。","categories":[{"name":"负载调度器","slug":"负载调度器","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E8%B4%9F%E8%BD%BD%E8%B0%83%E5%BA%A6%E5%99%A8/"}],"tags":[{"name":"负载均衡","slug":"负载均衡","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"代理","slug":"代理","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E4%BB%A3%E7%90%86/"},{"name":"Nginx","slug":"Nginx","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Nginx/"}],"author":"熊猫菌"},{"title":"Nginx 简单配置","slug":"Nginx 简单配置","date":"2021-05-18T15:10:44.408Z","updated":"2021-05-18T15:11:57.861Z","comments":true,"path":"2021/05/18/Nginx 简单配置/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/18/Nginx%20%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Nginx 简单配置配置介绍Nginx多模块下，完全依靠配置文件来进行配置。Nginx配置中文文档 Location 映射配置location [=|~|~*|^~] /uri/ &#123; … &#125; = 开头表示精确匹配 ^~ 开头表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。以xx开头 ~ 开头表示区分大小写的正则匹配，以xx结尾 ~* 开头表示不区分大小写的正则匹配，以xx结尾 !~ 和!~* 分别为区分大小写不匹配及不区分大小写不匹配的正则 / 通用匹配，任何请求都会匹配到。 配置文件解析123456789101112131415161718192021222324252627282930313233343536373839# 全局块# user nobodyworker_processes 1; # 工作线程，数量越大，可以支持的并发数越多，但是受硬件制约。# error_log logs/error.log;# error_log logs/error.log notice;# error_log logs/error.log info;#pid logs/nginx.pid;# EVENT事件块events &#123; worker_connections 1024; # 单Worker支持的最大连接数。&#125;# HTTP全局块 需要配置的块http &#123; include mime.types; default_type application/octet-stream; sendfile on; # 启用零拷贝 access_log /var/log/nginx/access.log; # 设定日志格式 keepalive_timeout 65; # 长连接保活时间 server &#123; listen 80; # 监听的端口号 server_name localhost; # 主机地址 location / &#123; # 代理的请求格式 /=所有，可以用正则表达式配置规则 alias /data/html/; # 目录别名，查找资源时直接按此目录查找，必须以/结尾 root /data/html/; # 目录根名，查找资源时将这个拼接在资源路径前，可选/结尾 index index.html index.htm; # 默认请求地址 &#125; error_page 500 502 503 504 /50x.html; # 异常界面，HTTP状态码路由 location = /50x.html &#123; root html; &#125; &#125;&#125; 正向代理配置1234567891011121314151617181920http &#123; include mime.types; default_type application/octet-stream; keepalive_timeout 65; # 长连接保活时间 server &#123; # 端口 listen 8080; # 地址 server_name localhost; # DNS解析地址 resolver 8.8.8.8; # 代理参数 location / &#123; # $http_host就是我们要访问的主机名 # $request_uri就是我们后面所加的参数 proxy_pass http://$http_host$request_uri; &#125; &#125;&#125; 反向代理配置主要修改HTTP块 123456789101112131415161718192021222324252627http &#123; include mime.types; default_type application/octet-stream; keepalive_timeout 65; # 长连接保活时间 server &#123; listen 80; # 监听的端口号 server_name 192.168.180.181; # 主机地址 location ~ /A/ &#123; # 代理的请求格式 以A为根目录结构的请求 root html; # 目录位置 proxy_pass http://192.168.180.182:8080; # HTTP代理的服务器A地址 index index.html index.htm; &#125; location ~ /B/ &#123; # 代理的请求格式 以B为根目录结构的请求 root html; proxy_pass http://192.168.180.183:8080; # HTTP代理的服务器B地址 index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 负载均衡配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#设定http服务器，利用它的反向代理功能提供负载均衡支持http &#123; #设定mime类型,类型由mime.type文件定义 include /etc/nginx/mime.types; default_type application/octet-stream; #设定日志格式 access_log /var/log/nginx/access.log; #省略上文有的一些配置节点 #。。。。。。。。。。 #设定负载均衡的服务器列表 upstream mysvr &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.8.1x:3128 weight=5; #本机上的Squid开启3128端口,不是必须要squid server 192.168.8.2x:80 weight=1; server 192.168.8.3x:80 weight=6; &#125; upstream mysvr2 &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.8.x:80 weight=1; server 192.168.8.x:80 weight=6; &#125; #第一个虚拟服务器 server &#123; #侦听192.168.8.x的80端口 listen 80; server_name 192.168.8.x; #对aspx后缀的进行负载均衡请求 location ~ .*.aspx$ &#123; #定义服务器的默认网站根目录位置 root /root; #定义首页索引文件的名称 index index.php index.html index.htm; #请求转向mysvr 定义的服务器列表 proxy_pass http://mysvr ; #以下是一些反向代理的配置可删除. proxy_redirect off; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， client_body_buffer_size 128k; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; &#125; &#125;&#125; 动静分离配置123456789101112131415161718192021222324252627282930http &#123; include mime.types; default_type application/octet-stream; keepalive_timeout 65; # 长连接保活时间 server &#123; listen 80; # 监听的端口号 server_name 192.168.180.181; # 主机地址 location / &#123; root e:wwwroot; index index.html; &#125; # 所有静态请求都由nginx处理，存放目录为html location ~ .(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123; root /data/asset/; # 资源目录 &#125; # 所有动态请求都转发给tomcat处理 location ~ .(jsp|do)$ &#123; proxy_pass http://192.168.180.182:8080; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 小结 工作进程配置越大性能越好吗？ 并不，合适最好，根据CPU核数选择，相等即可。 代理怎么配置？ 主要是HTTP块的配置，看官方文档。","categories":[{"name":"负载调度器","slug":"负载调度器","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E8%B4%9F%E8%BD%BD%E8%B0%83%E5%BA%A6%E5%99%A8/"}],"tags":[{"name":"实践过程","slug":"实践过程","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B/"},{"name":"Nginx","slug":"Nginx","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Nginx/"}],"author":"熊猫菌"},{"title":"LVS 简单配置","slug":"LVS 简单配置","date":"2021-05-17T15:58:06.024Z","updated":"2021-05-17T15:59:38.145Z","comments":true,"path":"2021/05/17/LVS 简单配置/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/17/LVS%20%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE/","excerpt":"","text":"LVS 简单配置DR模式配置DR模式的配置关键点： 局域网VIP隐藏 MAC修改转发 VIP隐藏 网卡配置多IP(注意掩码位)： ifconfig [网卡名]:[接口号] [IP地址] [netmask] [掩码地址] ifconfig [网卡名]:[接口号] [IP地址]/[掩码位] 移除配置： ifconfig [网卡名]:[接口号] down LVS的掩码地址为255.255.255.0是为了能正确接收局域网中交换机发送的数据包。其他服务器的掩码要为255.255.255.255是为了避免其eth0网卡与lo网卡的netmask都是255.255.255.0，导致2块网卡识别的网络号都是192.168.180.*，数据包从最近的网卡lo发送，目的是要从eth0网卡出去。 ARP MAC配置修改kernel parameter，主要目录/proc/sys/net/ipv4/conf/[all/ens32/lo/default]：arp_ignore：定义接收到arp请求时的响应级别。 0：只要本地配置的有相应地址，就给予响应。(默认) 1：仅在请求的目标MAC地址配置请求到达的接口上的时候，才会给予响应。 访问受限，不能写，直接利用重定向变更值。变更[eth0/all]2个网卡配置。 arp_announce：定义将自己地址向外通告时的通告级别。 0：将本地任何接口上的任何地址向外通告。(默认) 1：试图仅向目标网络通告与其网络匹配的地址。 2：仅向与本地接口上的地址匹配的网络地址通告。 访问受限，不能写，直接利用重定向变更值。变更[eth0/all]2个网卡配置。 演示配置windows上配置演示： yum install -y ipvsadm：安装lvs ipvsadm -A -t 192.168.180.233:80 -s rr：配置入口规则t=tcp，rr=轮询负载 ipvsadm -a -t 192.168.180.233:80 -r 192.168.180.182 -g -w 1：r=重定向，w[数字]=权重 ipvsadm -a -t 192.168.180.233:80 -r 192.168.180.183 -g -w 1：配置lvs到2台机器的负载映射 ipvsadm -ln：查看lvs配置路由规则，加参数c=查看路由记录表 映射的规则配置是为了获取到对应机器的MAC地址，方便LVS修改MAC后转发数据包。 小结 啊这，难搞。 那就用keepAlived吧，简化LVS的配置，并提供高可用保证。","categories":[{"name":"负载调度器","slug":"负载调度器","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E8%B4%9F%E8%BD%BD%E8%B0%83%E5%BA%A6%E5%99%A8/"}],"tags":[{"name":"实践过程","slug":"实践过程","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B/"},{"name":"LVS","slug":"LVS","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/LVS/"}],"author":"熊猫菌"},{"title":"LVS KeepAlived高可用方案","slug":"LVS KeepAlived高可用方案","date":"2021-05-17T15:58:06.023Z","updated":"2021-05-17T16:00:35.529Z","comments":true,"path":"2021/05/17/LVS KeepAlived高可用方案/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/17/LVS%20KeepAlived%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88/","excerpt":"","text":"LVS KeepAlived高可用方案KeepAlived介绍Keepalived是Linux下一个轻量级别的高可用解决方案。高可用(High Avalilability,HA)，其实两种不同的含义：广义来讲，是指整个系统的高可用行，狭义的来讲就是之主机的冗余和接管，它与HeartBeat RoseHA 实现相同类似的功能，都可以实现服务或者网络的高可用，但是又有差别，HeartBeat是一个专业的、功能完善的高可用软件，它提供了HA 软件所需的基本功能，比如：心跳检测、资源接管，检测集群中的服务，在集群节点转移共享IP地址的所有者等等。HeartBeat功能强大，但是部署和使用相对比较麻烦，与HeartBeat相比，Keepalived主要是通过虚拟路由冗余来实现高可用功能，虽然它没有HeartBeat功能强大，但是Keepalived部署和使用非常的简单，所有配置只需要一个配置文件即可以完成。 KeepAlived起初是为了LVS而设计，提供配置文件，后期通过vrrp协议(Vritrual Router Redundancy Protocol,虚拟路由冗余协议)允许与nginx，HaProxy等配合使用。KeepAlived有2种配置模式，抢占式和非抢占式。 LVS主备模式(抢占式)抢占式就是主机一旦挂掉，备机立刻会抢主机的位置，自身成为新主机。 主备模式演示： 主机备机配置文件： 12345678910111213141516171819202122232425262728293031323334vrrp_instance VI_1 &#123; state MASTER # 主机：MASTER，备机：BACKUP。其他一样。 interface eth0 # 需要监测的网卡 virtual_router_id 21 # 让主机和备机在同一个虚拟路由下，ID必须相同 priority 100 # 优先级，决定选择的优先级，越大优先级越高 advert_int 1 # 心跳间隔时间，单位秒 authentication &#123; auth_type PASS # 认证 auth_pass 1234 # 密码 &#125; vitual_ipaddress &#123; # 只有主机有VIP，备机没有，路由 192.168.180.233/24 dev eth0 label eth0:1 # LVS的VIP/NetMask dev 网卡名 label 网卡名：子接口号 &#125;&#125;virtual_server 192.168.180.233 80 &#123; # LVS的VIP + 实际端口 delay_loop 6 # lb_algo rr # 负载调度算法，rr=轮询 lb_kind DR # 工作模式 nat_mask 255.255.255.0 # 掩码 persistence_timeout 50 # 负载连接维护时长，单位秒 protocol TCP # 传输协议 real_server 192.168.180.182 80 &#123; # 真实服务器的IP与端口，可以配置多个真实服务器。 weight 1 # 权重 HTTP_GET &#123; # 健康检查形式，HTTP_GET，SSL_GET url &#123; # 访问确认地址，可以配置多个。 path / # 访问地址，此处为访问服务器根地址。 status_code 200 # 如何判定成功，此处为响应状态码为200时认为ok。 &#125; connect_timeout 3 # 连接超时时间 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125; 存在不可靠的问题： 如果KeepAlived应用异常退出，导致主机的VIP可能没有被收回，LVS映射MAC地址也没有收回。然后备机发现无法联通主机，备机自动抢占为主机，但是这时局域网内就存在了2个相同的暴露出去的VIP。数据包发送就不确定了，不可靠。 LVS集群模式(非抢占式)配置文件上区别不大，多了一个属性：nopreempt：标识为非抢占式，也就是投票选择主机，公平模式。 引入zookeeper来保证KeepAlived高可用应用集群的可靠性。 KeepAlived脑裂现象脑裂现象一般发生在抢占式中。由于某些原因，导致两台KeepAlived高可用服务器在指定时间内，无法检测到对方的存货心跳信息，从而导致互相抢占对方的资源和服务所有权，然后两台却还都存活中。 脑裂示意图： 脑裂发生的原因： 开启了防火墙，限制了对方的访问 网线故障，局域网不连通 心跳时间极短，这个应该不会","categories":[{"name":"负载调度器","slug":"负载调度器","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E8%B4%9F%E8%BD%BD%E8%B0%83%E5%BA%A6%E5%99%A8/"}],"tags":[{"name":"实践过程","slug":"实践过程","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B/"},{"name":"LVS","slug":"LVS","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/LVS/"}],"author":"熊猫菌"},{"title":"LVS 基础知识","slug":"LVS 基础知识","date":"2021-05-14T17:30:37.926Z","updated":"2021-05-19T13:52:30.640Z","comments":true,"path":"2021/05/15/LVS 基础知识/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/15/LVS%20%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"LVS 基础知识LVS 介绍LVS(Linux Virtual Server)即Linux虚拟服务器，通常是实现虚拟网络服务负载调度的主要技术，而且是目前IP负载调度器中实现技术中效率最高的。在已有的IP负载均衡调度器技术中，主要有通过网络地址转换(Network Address Translation)将一组服务器构成一个高性能、高可用的虚拟服务器，也就是VS/NAT技术。在分析VS/NAT的缺点和网络服务的非对称性的基础上，又提出了通过IP隧道实现虚拟服务器的方法VS/TUN，和通过直接路由实现虚拟服务器的方法VS/DR，他们可以极大的提高系统的伸缩性。这三种技术是LVS集群中实现的三种IP负载均衡技术。 LVS 优点 抗负载能力强，工作在传输层上仅分发，没有流量的产生，它在所有负载均衡软件里的性能最强，对于内存和CPU资源消耗较低。 配置性比较低，相比其他没有过多的配置，不容易出错。 工作稳定，本身抗负载能力很强，自身有完整的双机热备方案(LVS KeepAlived主备模式)。 无流量，无需建立连接，只分发数据包，保证IO的性能不会受到大流量影响。 应用范围广，工作在传输层，可以对几乎所有应用负载均衡。 用来顶住前端的大流量访问压力。 LVS 缺点 因为没有解析到上层，所以要求被负载的服务器一定是镜像服务器。 软件本身不支持正则表达式处理，不能做动静分离。 对于大型应用体系下，单纯依靠LVS KeepAlived实现复杂。 网络模型： 本身比路由器多了一层传输层，但是却并不是完整的传输层，因为传输层应该是建立连接的(TCP)，但是LVS并没有建立连接，仅仅解析出访问的端口，就直接回到网络层-链路层-物理层然后转发出去，解析端口的目的是为了识别出同一种服务，然后做负载均衡。 网络拓扑图： 由于LVS不解析到内容，所以代理的服务器A和服务器B应该是同一个服务，是同一个，如果A和B不同，那么客户端在访问的时候，如果期望是访问A的服务，那么给客户端的感觉就是时好时坏。 LVS 工作模式LVS有三种工作模式，分别具有不同的应用场景和特点。 VS/NAT模式网络地址转换模式，就类似于一个路由器，将访问地址替换为目标地址。回去时再做一次转换。 NAT过程示意图： 客户端发送数据包到LVS上，此时数据包源IP=CIP，目标IP=VIP。 LVS外部网卡接收到数据包后，根据内部网卡连接的服务器，选择一台的RIP进行替换IP并生成虚拟端口。 将CIP-RIP的实际连接和他们的端口维护在自己的路由表中。 数据包替换完成后转发给对应的内部服务器(也可以是另外的路由器或交换机或负载均衡器)，此时源IP=CIP，目的IP=RIP。 内部服务器响应数据包，此时源IP=RIP，目的IP=CIP，原样返回。 LVS接收到数据包响应后，先去自己的路由表中查找，然后将RIP替换为VIP(端口也还原)，此时源IP=VIP，目的IP=CIP。 客户端(可能也是路由器或交换机等)接收到数据包后，发送的时候源IP=CIP，目的IP=VIP，接收源IP=VIP，目的IP=CIP，可以接收。 从流程中可以看出，NAT模式中LVS是所有网络连接的通讯桥梁。每次都需要LVS进行解包，改包，发送，回去的时候，又要解包，改包，发送。优点： 实现相对简单。 缺点： 服务器带宽瓶颈：每次都要经过它。 消耗CPU的计算力：每次都要解包，改包，发包。 VS/TUN模式为了解决NAT的带宽瓶颈，和CPU的多次计算，不采用每次都走LVS的模式，所以TUN模式下的LVS就是一个领路人，只负责把客户端的数据包路由到对应的服务器上，响应就让对应服务器直接丢回去就行，不需要再走LVS了，这就是IP隧道模式。 TUN过程示意图： 客户端发送数据包到LVS上，此时数据包源IP=CIP，目标IP=VIP。 LVS外部网卡接收到数据包后，根据内部网卡连接的服务器，选择一台的RIP进行再次封包，加一层。 数据包封装完成后转发给对应的内部服务器，此时源IP=DIP，目的IP=RIP，内层数据包源IP=CIP，目的IP=VIP。 内部服务器接收到数据包后解出DIP-RIP的数据包，然后继续解出内部CIP-VIP的数据包进行处理。 处理完成后直接根据隐藏的VIP回复VIP-CIP的数据包回去，不需要封到RIP-DIP的层次。 客户端(可能也是路由器或交换机等)接收到数据包后，发送的时候源IP=CIP，目的IP=VIP，接收源IP=VIP，目的IP=CIP，可以接收。 从流程中可以看出，TUN模式中LVS只对请求做了处理，而且是加了一层数据包的包装后，然后发送数据包，响应的时候无需经过LVS处理。优点： 缓解瓶颈：不需要处理响应数据，只需要处理请求数据。 缓解CPU的计算力：LVS本身做的事情比较少。 缺点： 要求硬件支持IP隧道协议：利用的IP隧道的方式多封装一次包，服务器需要多解一次包。 配置复杂：需要配置好隐藏IP，隐藏IP不能是具有ARP的设备，也就是不能有实际的MAC，否则会暴露。 VS/DR模式为了平衡NAT和TUN模式，DR模式可以说结合了两者的优势，屏蔽了一些缺陷。 DR过程示意图： 客户端发送数据包到LVS上，此时数据包源IP=CIP，目标IP=VIP。 LVS外部网卡接收到数据包后，根据内部网卡连接的服务器，选择一台的RIP的MAC地址替换，并广播回局域网中。 内部服务器发现MAC与自己一致，二层验证通过接收，发现自己的内部网卡VIP与数据包目的IP一致，接收。 服务器处理完成后，直接通过内部网卡返回数据包，此时源IP=VIP，目的IP=CIP。 客户端(可能也是路由器或交换机等)接收到数据包后，发送的时候源IP=CIP，目的IP=VIP，接收源IP=VIP，目的IP=CIP，可以接收。 从流程中可以看出，DR模式是基于二层来直接修改MAC的，进而来控制数据包的分发，但是由于是LAN数据包发送，所以也有一定的限制，要求LVS与真实服务器在同一个局域网下。优点： 速度极快：基于二层的代理，直接修改MAC。 结合了NAT与TUN的优点。 缺点： LVS与真实服务器必须位于统一局域网：因为是直接基于MAC的，所以不能走路由设备，否则内部lo网卡无法配置相同的VIP。 LVS 负载策略不同的工作模式下都需要根据一定的策略去选择如何负载，依靠负载策略去选择合适的RIP或MAC。 轮询调度(Round-Robin Scheduling)循环调度算法将每个传入请求发送到列表中的下一个服务器。因此，在三个服务器群集（服务器A，B和C）中，请求1将转到服务器A，请求2将转到服务器B，请求3将转到服务器C，请求4将转到服务器A，从而完成循环或“轮循”服务器。它将传入的连接数或每台服务器遇到的响应时间视为平等，将所有真实服务器视为相等。与传统的轮询DNS相比，虚拟服务器具有一些优势。轮询DNS将单个域解析为不同的IP地址，调度粒度基于主机，并且DNS查询的缓存阻碍了基本算法，这些因素导致实际服务器之间出现显着的动态负载不平衡。 加权轮询调度(Weighted Round-Robin Scheduling)加权循环调度旨在更好地处理具有不同处理能力的服务器。可以为每个服务器分配一个权重，这是一个指示处理能力的整数值。权重较高的服务器比权重较小的服务器首先获得新的连接，权重较高的服务器比权重较小的服务器获得更多的连接，权重相等的服务器获得相等的连接。例如，真实服务器A，B和C分别具有权重4，4，3，在调度周期（mod sum（Wi））中，一个好的调度序列将是AABABCABC。在加权循环调度的实现中，修改虚拟服务器的规则后，将根据服务器的权重生成调度序列。 当真实服务器的处理能力不同时，加权轮询调度比轮询调度要好。但是，如果请求的负载变化很大，则可能导致实际服务器之间的动态负载不平衡。简而言之，有可能将大多数需要较大响应的请求都定向到同一台真实服务器。 实际上，循环调度是加权循环调度的一个特殊实例，其中所有权重均相等。 最少连接调度(Least-Connection Scheduling)最少连接调度算法将网络连接以最少的已建立连接数定向到服务器。这是动态调度算法之一；因为它需要动态计算每个服务器的活动连接数。对于正在管理具有相似性能的服务器集合的虚拟服务器，当请求的负载变化很大时，最少连接调度可以很好地平滑分发。虚拟服务器将以最少活动连接将请求定向到真实服务器。 乍一看，即使存在具有各种处理能力的服务器，最小连接调度也似乎可以很好地执行，因为速度更快的服务器将获得更多的网络连接。实际上，由于TCP的TIME_WAIT状态，它不能很好地执行。TCP的TIME_WAIT通常为2分钟，在这2分钟内，繁忙的网站通常会收到数千个连接，例如，服务器A的功能是服务器B的两倍，服务器A正在处理数千个请求并将其保存在服务器中。 TCP的TIME_WAIT状态，但是服务器B正在爬网以完成其数千个连接。因此，最少连接调度无法在具有各种处理能力的服务器之间很好地平衡负载。 加权最少连接调度(Weighted Least-Connection Scheduling)加权的最小连接调度是最小连接调度的超集，您可以在其中为每个真实服务器分配性能权重。权重值较高的服务器将在任何时候获得较大比例的实时连接。虚拟服务器管理员可以为每个真实服务器分配权重，并为每个服务器安排网络连接，其中每个服务器当前活动连接数的百分比与其权重之比。默认权重是1。 加权最小连接调度的工作方式如下：假设有n个真实服务器，每个服务器i的权重为Wi(i = 1，..，n)，活动连接为Ci(i = 1，..，n)，则ALL_CONNECTIONS是Ci(i = 1，..，n)，下一个网络连接将定向到服务器j，其中(Cj / ALL_CONNECTIONS) / Wj = min{(Ci / ALL_CONNECTIONS) / Wi}(i = 1，..，n)，由于ALL_CONNECTIONS在此查找中是一个常量，因此无需将Ci除以ALL_CONNECTIONS，因此可以将其优化为Cj / Wj = min{Ci / Wi}(i = 1，..，n)，加权的最小连接调度算法需要比最小连接更多的划分。为了最大程度地减少服务器具有相同处理能力时的调度开销，同时实现了最小连接调度和加权最小连接调度算法。 基于位置的最少连接调度(Locality-Based Least-Connection Scheduling)基于位置的最小连接调度算法用于目标IP负载平衡。它通常在高速缓存群集中使用。如果服务器处于活动状态且处于负载状态，此算法通常会将发往IP地址的数据包定向到其服务器。如果服务器过载（其活动连接数大于其权重），并且服务器处于半负荷状态，则将加权最少连接服务器分配给该IP地址。 具有复制调度的基于位置的最少连接调度(Locality-Based Least-Connection with Replication Scheduling)具有复制调度算法的基于位置的最小连接也用于目标IP负载平衡。它通常在高速缓存群集中使用。它与LBLC调度有以下不同：负载平衡器维护从目标到可以为目标提供服务的一组服务器节点的映射。对目标的请求将分配给目标服务器集中的最少连接节点。如果服务器集中的所有节点都超载，则它将拾取群集中的最小连接节点，并将其添加到目标服务器群中。如果在指定时间内未修改服务器集，则从服务器集中删除负载最大的节点，以避免高度复制。 目标哈希调度(Destination Hashing Scheduling)目标哈希调度算法通过根据服务器的目标IP地址查找静态分配的哈希表来将网络连接分配给服务器。 源哈希调度(Source Hashing Scheduling)源哈希调度算法通过根据服务器的源IP地址查找静态分配的哈希表来将网络连接分配给服务器。 最短预期延迟调度(Shortest Expected Delay Scheduling)最短的预期延迟调度算法将网络连接分配给具有最短的预期延迟的服务器。如果将作业发送到第i个服务器，则预期的工作延迟为（Ci +1）/ Ui，其中Ci是第i个服务器上的连接数，而Ui是第i个服务器的固定服务速率（权重） 。 永不排队调度(Never Queue Scheduling)从不排队调度算法采用两速模型。当有空闲服务器可用时，作业将被发送到空闲服务器，而不是等待快速的服务器。当没有可用的空闲服务器时，作业将被发送到服务器，以使其预期延迟最小化（最短预期延迟调度算法）。 小结 目前采用得多的是？ 要么是NAT，要么是DR。毕竟越简单的越好，容错率越高越好。 调度策略怎么选？ 按合适的选…","categories":[{"name":"负载调度器","slug":"负载调度器","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E8%B4%9F%E8%BD%BD%E8%B0%83%E5%BA%A6%E5%99%A8/"}],"tags":[{"name":"LVS","slug":"LVS","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/LVS/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"代理","slug":"代理","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E4%BB%A3%E7%90%86/"}],"author":"熊猫菌"},{"title":"Kafka 进阶知识","slug":"Kafka 进阶知识","date":"2021-05-12T16:11:16.977Z","updated":"2021-05-13T15:10:40.157Z","comments":true,"path":"2021/05/13/Kafka 进阶知识/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/13/Kafka%20%E8%BF%9B%E9%98%B6%E7%9F%A5%E8%AF%86/","excerpt":"","text":"Kafka 进阶知识Kafka 日志截断 HW(High Watermarker)：高水位线。所有HW之前的数据都理解为是已经备份的，当所有节点都备份成功，Leader会更新水位线。 ISR(In-Sync-Replicas)：正在同步中的副本。Kafka的Leader会维护一份处于同步的副本集合，如果在replica.lag.time.max.ms时间内系统没有发送fetch请求，或者依然在发送请求，但是在该限定时间内没有赶上Leader的数据就会被踢出ISR列表。(0.9.0后已废弃的配置：replica.lag.max.messages消息个数限定，这个会导致其他Broker节点频繁加入和退出ISR) LEO(Log End Offset)：日志标识。log and offset标识的是每个分区中最后一条消息的下一个位置，分区中的每个副本都有自己的LEO。 Kafka 高水位Kafka中的Topic被分为多个分区，分区是按照Segments存储文件块。分区日志是存储在磁盘上的日志序列，Kafka可以保证分区里的事件是有序的。其中Leader负责对应分区的读写，Follower负责同步分区的数据。但是在0.11前，使用HW保证数据的同步，但是基于HW的数据同步可能会导致数据的不一致或者是乱序。 消息同步方式：高水位的更新： 1,2…：代表了上图中的segment。HW：就是所有副本都同步的公共的位置。LEO：每个副本的最后一个同步位置。 消息丢失案例（0.11以前的版本，使用需谨慎）： 副本同步Leader数据m2时，还没有更新自己的HW。 Leader确认副本已经同步过m2，所以主机更新了自己HW=1。 副本突发故障，重启后发现自己的HW=0，丢弃1位置的数据。 副本准备连接Leader确认自己的水位线时，Leader也故障。 副本被选为Leader，导致1的位置数据变为m3，从而丢失了m2数据。 数据不一致问题（0.11以前的版本，使用需谨慎）： 副本数据只同步到m1，HW=0，Leader的数据写到m2，HW=1。 副本和Leader同时故障。 副本先行重启，旧Leader没有重启，副本被选为新Leader。 新Leader收到一条记录m3写入，更新HW=1。 旧Leader重启，向新Leader同步数据，发现HW=1，认为已经同步，实际数据不一致。 Kafka EpochEpoch解决高水平截断问题（0.11+之后的改进） 控制器controller负责管理epoch信息，并存储在zookeeper中。 控制器将epoch信息同步给Leader，也就是LeaderEpoch。 每次Leader接收到Producer的消息时，使用LeaderEpoch标记Message。 副本主动同步获取LeaderEpoch编号，替换HW的标记，作为消息的截断。 该epoch信息还会随着每一次LeaderAndIsrRequest传递给每一个新的Leader。 改进点：消息格式改进，每个消息集都带有一个4字节的Leader Epoch号。在每个日志目录中，会创建一个新的Leader Epoch Sequence文件，在其中存储Leader Epoch的序列和在该Epoch中生成的消息的Start Offset。它也缓存在每个副本中，同时还缓存在内存中。 Follower -&gt; Leader：首先将新的Leader Epoch和副本的LEO添加到Leader Epoch Sequence序列文件的末尾并刷新数据。给Leader产生的每个新消息集都带有新的Leader Epoch标记。 Leader -&gt; Follower：如果需要从本地的Leader Epoch Sequence加载数据，将数据存储在内存中，给相应的分区的Leader发送Epoch请求，该请求包含最新的EpochID，Start Offset信息(历史信息)。Leader接收到信息以后返回该EpochID所对应的Last Offset信息。该信息可能是最新的EpochID的Start Offset或者是当前EpochID的Log End Offset信息。 Kafka EagleKafka监视系统，在github上直接安装Kafka-eagle-web就可以。Kafka-Eagle Github Kafka StreamingKafka Streaming也是一种实时在线流处理框架，相比较于其他的专业流处理框架Storm，Spark Stream，Flink等，它运行于应用端，不需要独立的服务器去运行它，具有简单，入门要求低，部署方便等优点。 特点 Kafka Streaming提供了一个简单轻量级的lib库，可以非常方便的集成到任何Java项目中。 除了Kafka外，没有任何外部依赖。 利用Kafka的分区模型支持水平拓展和保证顺序性。 通过可容错的state store实现高效的状态操作(windowed joins and aggregations)。 支持exactly once一次处理语义(即幂等写)。 支持消息记录级的处理，实现毫秒级的延迟。 提供High-Level的Stream DSL(类似于Spark的map/group/reduce)和Low-Level的Processor API(类似于Storm的spout和bolt)。 流处理拓扑 Stream(流)：流是Kafka Streams中最重要的抽象，代表了一个无界的，不断更新的数据集。流是不可变数据消息的有序、可重播和容错序列，其中数据被定义为键值对。 Stream Processing Application(流处理应用)：是使用了Kafka Streams库的应用，它通过定义一个或多个**processor topologies(处理器拓扑)**定义其计算逻辑，其中处理器拓扑是由流(边缘)连接的流处理器(节点)的图。 Stream Processor(流处理器)：是拓扑流图中的一个节点，表示了一个处理步骤，通过一次从拓扑中的上游处理器接收一个输入消息，对其应用操作，来转换流中的数据，并且随后可以向其下游处理器生成一个或多个输出消息。 拓扑中有2个特殊的处理器： Source Processor(源处理器)：源处理器没有任何上游处理器，通过使用来自一个或多个Kafka Topic的记录，并将这些消息转发到其下游处理器，从而从一个或多个Kafka Topic生成一个输入到其拓扑的流。 Sink Processor(接收器处理器)：接收器处理器没有任何的下游处理器，它将所有从上游处理器接收到的消息发送到指定的Kafka Topic。 Normal Processor(普通处理器)：普通处理器既可以在处理中访问其他远程系统，处理后的结果可以流回Kafka或写入外部系统。 处理器拓扑图： Kafka Streams提供了2种方法来定义流处理拓扑结构： Kafka Streams DSL：上层提供了常用的数据转换操作，例如map，filter，join和aggregations。 Processor API：下层允许开发人员自定义处理器以及与state store进行交互。 时间流处理中一个关键方面是时间的概念，以及如果对其进行建模和集成。例如，某些操作(Windowing)是基于时间边界定义的。流中常见的时间概念是： Event Time：事件时间即事件或消息发生的时间点，最初是发生在源头上的时间。 Processing Time：处理时间即事件或消息恰好由流处理应用程序处理的时间点，也就是消费消息时的时间点，处理时间可能比原始事件时间晚。 Ingestion Time：接收时间即Kafka Broker将事件或数据消息存储在Topic分区中的时间点，与Event Time的差异在于，Ingestion Time是在Kafka Broker将消息添加到目标Topic时生成的，不是在源头创建消息时生成的。一条没有被处理过的消息没有Processing Time，但是有Ingestion Time。 Event Time和Ingestion Time的选择实际上是通过配置Kafka完成的：从Kafka0.10.x起，时间戳自动嵌入到Kafka消息中，即入门里介绍的timestamp，这些时间戳表示Event Time或Ingestion Time。可以在Broker级别或每个Topic上上指定相应的Kafka配置。每当Kafka Streams应用将记录写入Kafka时，它将为这些新消息分配时间戳，分配方式取决于上下文： 当通过处理某些消息(例如context.forward()在process()函数调用中触发)来生成新的输出消息时，输出消息时间戳会直接从输入消息时间戳继承。 当通过周期性函数来生成新的输出消息时(Punctuator#punctuate())，输出消息时间戳定义为context.timestamp()流任务的当前内部时间。 对于aggregation将是取所有消息中最大的时间戳。 对于aggravation，join时间戳的计算方式使用一下规则： 对于具有左右输入记录的join(stream-stream, table-table)，将为输出消息的时间戳分配为max(left.ts, right.ts)。 对于stream-join，将为输出消息的时间戳分配为stream的时间戳。 对于aggregation，针对全局(非窗口式)或每个窗口内计算所有消息上max的时间戳。 对于无状态操作，将传递输入消息的时间戳。 流表二重性几乎任何流处理技术都需要为流和表提供支持，因为大部分情况下，我们都是需要数据库的。Kafka提供了流表二重性的性质，即流可以当做表来看待，表也可以当做流来处理。 流表二重性： 流 -&gt; 表：流可以视为表的更改日志，流中的每个消息都捕获表的状态更改。因此流是变相的表，并且通过从头到尾重现更改日志以重建表，可以很容易的将其变成表。 表 -&gt; 流：表在某个时间点可以视为流中的每个键的最新值的快照(流的数据记录是键值对)。因此，表是变相的流，并且可以通过迭代表中的每个键值条目将其轻松转换为流。 Aggregation一个聚合操作需要一个输入流或表，并且由多个输入消息组合为单个输出消息产生的新表。 Windowing窗口允许用户控制以具有相同密钥组消息的状态操作，如aggregations或joins窗口。使用窗口时，可以为窗口指定grace period，它控制Kafka Streams将等待给定窗口多长时间的无序消息记录。如果超过了该期限，则消息记录被丢弃，并且不会在窗口中进行处理。 State StoreKafka Streams提供了State Store，可以由流处理应用使用它来存储和查询数据。在执行有状态操作时，这是很重要的功能。可以通过API访问state store来存储和查询所需要的数据。Kafka Streams为本地state store提供容错和自动恢复。 乱序处理在Kafka Streams中，有两个原因可能导致数据时间戳相对于它们的到达顺序混乱： 在主题分区中，消息记录的时间戳及其offset可能不会单调增加。由于Kafka Streams始终会尝试按照offset顺序处理topic分区中的消息记录，因此它可能导致在相同topic中具有较大时间戳（但offset较小）的消息比具有较小时间戳（但offset较大）的消息要早处理。 在可能正在处理多个topic分区的流任务中，如果用户将应用程序配置为不等待所有分区都包含一些缓冲的数据，并从时间戳最小的分区中选取来处理下一条消息记录，则稍后再处理某些消息记录时如果是为其他topic分区获取的，则它们的时间戳可能小于从另一个topic分区获取的已处理消息记录的时间戳。 对于无状态操作，无序数据不会影响处理逻辑，因为一次只考虑一个消息，而无需查看过去处理消息的历史记录；但是对于有状态操作(例如aggregation和join)，乱序数据可能会导致处理逻辑不正确。如果用户希望处理此类乱序数据，则通常需要允许其应用程序等待更长的时间，同时在等待时间内记录其状态，即在延迟，成本和正确性之间做出权衡决策。但是对于join，某些乱序数据无法通过增加Streams的延迟和成本来处理： 对于Stream-Stream连接：所有三种类型(内部，外部，左)都可以正确处理乱序记录，但是对于左联接，结果流可能包含不必要的leftRecord-null;对于外部联接，结果流可能包含不必要的leftRecord-null或null-rightRecord。 对于Stream-Table连接：不处理无序记录(即，Streams应用程序不检查无序记录，而仅以偏移顺序处理所有记录)，因此可能会产生不可预测的结果。 对于Table-Table连接：不处理无序记录(即，Streams应用程序不检查无序记录，而仅以偏移顺序处理所有记录)。但是，联接结果是变更日志流，因此最终将保持一致。 基本架构架构示意图： 流分区(Stream Partitions)和任务(tasks)Kafka的消息传递层对数据进行分区以进行存储和传输。Kafka Streams对数据进行分区以进行处理。在这两种情况下，这种分区都可以实现数据局部性，弹性，可伸缩性，高性能和容错能力。Kafka Streams使用分区和任务的概念作为基于Kafka Topic分区的并行模型的逻辑单元。在并行性方面，Kafka Streams和Kafka之间存在紧密的联系： 每个流分区都是数据消息记录的完全有序序列，并映射到Kafka Topic分区。 一个数据消息记录的流映射到Kafka Topic中的一个消息。 数据消息记录的keys决定了Kafka流和Kafka流中的数据分区，即如何将数据路由到Topic内的特定分区。 线程Kafka Streams允许用户配置该库可用于并行化应用程序实例中的处理的线程数。每个线程可以使用其处理器拓扑独立执行一个或多个任务。 容错能力Kafka Streams建立在Kafka本地集成的容错功能的基础上。对于每个State Store，它维护一个复制的变更日志Kafka Topic，在其中跟踪任何状态更新。这些变更日志Topic也已分区，因此每个本地State Store实例以及访问该存储的任务都有自己的专用变更日志Topic分区。如果任务在发生故障的计算机上运行并在另一台计算机上重新启动，则Kafka Streams通过在恢复对新启动的任务的处理之前重现相应的变更日志Topic分区，来保证将其关联的State Store恢复到故障之前的内容。所以故障处理最终对用户是完全透明的。 小结 高水位截断的问题？ 主要是0.11版本之前，会发生数据丢失和不一致的问题。主要发生在同步过程。（0.11+版本epoch已解决） Kafka Streams的能否处理乱序数据？ 可以处理，但是对于Stream-Stream和Stream-Table下的join可能存在一定的风险，需要考虑，Table-Stream也仅能保证最终一致性。 学习自：Kafka官方文档","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Kafka/"},{"name":"流处理","slug":"流处理","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E6%B5%81%E5%A4%84%E7%90%86/"}],"author":"熊猫菌"},{"title":"Kafka API使用","slug":"Kafka API使用","date":"2021-05-12T14:39:58.202Z","updated":"2021-05-12T16:23:46.186Z","comments":true,"path":"2021/05/12/Kafka API使用/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/12/Kafka%20API%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Kafka API使用Kafka基础APIKafka常用API介绍 KafkaAdminClientAdminClientConfig.BOOTSTRAP_SERVERS_CONFIG：Kafka服务器地址。 create(Properties props)：创建一个Kafka客户端连接。 listTopics()：获取所有的topic主题。 createTtopics(Collection&lt;NewTopic&gt; topics)：批量创建topic。这个方法是一个异步方法，所以我们创建后的主题无法立刻查询到。可以使用创建后的主题结果createTopicResult.all().get()来同步等待主题的创建。 deleteTopics(Collection&lt;String&gt; topics)：批量删除topic。也是异步方法，同理也可以同步去等待执行完成。 describeTopics(Collection&lt;String&gt; topics)：批量查看topic的详细信息。得到一个DescribeTopicsResult。 close()：关闭连接。 KafkaProducerProducerConfig.BOOTSTRAP_SERVERS_CONFIG：Kafka服务器地址，作为生产者使用。ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG：key的序列化方式，可以自定义实现序列化方式。ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG：value的序列化方式，可以自定义实现序列化方式。ProducerConfig.PARTITION_CLASS_CONFIG：自定义分区，需要实现Partitioner接口。生产者投递消息时，默认是采用轮询的方式投递消息。 构造：构造出将准备向某一个topic投递的生产者。 send(ProducerRecord&lt;String, String&gt; record)：发送一个消息，消息有不同的构造，可以自己定义key,value和时间戳。 flush()：清理数据发送缓冲区。 close()：关闭生产者连接。 KafkaConsumerConsumerConfig.BOOTSTRAP_SERVERS_CONFIG：Kafka服务器地址，作为消费者使用。ConsumerConfig.KEY_SERIALIZER_CLASS_CONFIG：key的序列化方式，可以自定义实现序列化方式。ConsumerConfig.VALUE_SERIALIZER_CLASS_CONFIG：value的序列化方式，可以自定义实现序列化方式。ConsumerConfig.GROUP_ID_CONFIG：消费者组的ID。 subscribe(Pattern reg)：订阅对应正则表达式下的主题，当然可以订阅多个。 assign(List&lt;TopicPartiton&gt; topicPartitions)：订阅一个topic的某些分区。 poll(Duration timeout)：多长时间去抓取消息记录，获取一个ConsumerRecord消息。 ProducerInterceptor onSend(ProducerRecord record)：用户在向topic中投递信息后可以进行一些处理。 onAcknowledgement(RecordMetaData data, Exception ex)：data是发送成功时的topic消息处理的回调信息。 close()：不咋用 configure(Map&lt;String, ?&gt; configures)：不咋用 Kafka高级APIOffset自动控制Kafka消费者对于未订阅的topic的offset的时候，也就是系统并没有存储该消费者的消费分区的记录信息，默认Kafka消费者的首次消费策略是：latest。auto.offset.reset = latest earliest：自动将偏移量重置为最早的偏移量。 latest：自动将偏移量重置为最新的偏移量。 none：如果没有找到消费者组的先前偏移量，则会向消费者抛出异常。 Kafka消费者在消费数据的时候默认会定期的提交消费的偏移量，这样就可以保证所有的消息至少可以被消费者消费1次，可以通过一下参数来进行配置：enable.auto.commit = trueauto.commit.interval.ms = 5000 如果需要自己手动控制offset的提交，可以关闭offset的自动提交，但是要注意手动提交时，提交的偏移量永远要比本次消费的偏移量+1，因为提交的偏移量是Kafka消费者组下一次将要获取的数据位置。 KafkaConsumerConsumerConfig.AUTO_OFFSET_RESET_CONFIG：手动去配置offset的提交方式，默认是latest。ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG：offset的自动提交间隔，默认是5s。ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG：是否开启自动提交，默认是true。 commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)：异步手动提交偏移量信息。 Ackes&amp;RetriesKafka生产者在发送完一个消息后，要求Broker在规定的时间内应答Ack，如果没有应答，那么Kafka生产者会尝试在n次内重新发送消息。acks = 1 1：Leader会将Record写到其本地日志中，但会在不等待所有Follower的完全确认的情况下做出响应。在这种情况下，如果Leader在确认记录成功后立即宕机，发生在Follower复制记录之前，则记录丢失。 0：生产者根本不会等待服务器的任何确认，该记录将立即发送添加到套接字缓冲区中并视为已发送。在这种情况下，不能保证服务器已收到记录。不可靠。 -1(all)：意味着Leader将等待全套同步副本确认记录。这保证了只要至少一个同步副本仍处于活动状态，记录就不会丢失，这是最有力的保证。慢。 如果生产者在规定的时间内，并没有得到Kafka的Leader的Ack应答，Kafka可以开启retries机制，重试。request.timeout.ms = 30000retries = 2147483647 应答过程时序： Leader在记录完成后，响应丢失，重试时发生消息记录重复。 KafkaProducerProducerConfig.ACKS_CONFIG：acks配置，默认是1。ProducerConfig.RETRIES_CONFIG：重试的最大次数，默认是2147483647次，几乎无限重试。ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG：每次重试的超时检测时间，默认是30s。 幂等写幂等是针对生产者角度的特性，可以保证生产者发送的消息不会丢失，不会重复，实现的关键点是服务端可以区分请求是否重复，过滤掉重复的请求。但是只能保证在一个分区内的消息发送的原子性。 唯一标识：要想区分请求是否重复，请求中就得有唯一标识。 记录下处理过的请求标识：每次接收到新请求时，会把请求标识与处理记录做比较，如果是重复请求，就直接拒绝掉。 幂等又称为exactly once。要停止多次处理同一消息，必须将其持久化到Kafka Topic中仅一次，在初始化期间，Kafka会给每个生产者生成一个唯一的ID称为Producer ID或PID。PID和序列号与消息捆绑在一起，然后发送给Broker。由于序列号从零开始且单调递增，因此，仅当消息的序列号比该PID/TopicPartition对中最后提交的消息正好大1时，Broker才会接受该消息，否则Broker认为是重复消息。enable.idempotence = false注：如果要开启幂等性，要求必须开启acks = all和retries = true。 消息重复的解决： KafkaProducerProducerConfig.ENABLE_IDEMPOTENCE_CONFIG：幂等性配置，默认是false。(为true时一定要acks=all和retries=true)ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PRE_CONNECTION：最大允许等待数，设置为1时，可以保证只要有1个没有确认成功，就会阻塞，不会接收下一个，保证有序。 生产者事务对比幂等性，幂等只能保证一个分区内的消息的发送的原子性，而Kafka的事务操作可以保证在一个topic内的多条消息即多个分区间的消息完整性。事务通常分为2类： 生产者事务：仅仅控制生产者的一些特性，即消息的发送性质。批量提交类似DB，只要有一个失败即全部失败。标记失败的数据。 生产者&amp;消费者事务：即充当消费者又充当生产者的情况，保证从消息的消费到消息的发送的整个流程的原子性。包括消费时offset的撤销和发送的数据标记。isolation.level = read_uncommitted read_committed：读已提交的数据，开启事务时，消费者端一定是开启read_committed的，不然就没有意义了。 read_uncommitted：读未提交的数据，就相当于不开启事务。 Kafka事务与数据库事务不太一样的地方：当事务失败时，数据并不会回滚，仍然会把消息写入到topic的分区中，但是事务失败的时候会将消息标记为事务失败，因此如果开启了事务的隔离别，就可以防止消费者读到这些已经被标记为失败的数据。开启生产者事务的时候，只需要指定transactional.id属性即可，一旦开启了事务，默认生产者就会自动开启幂等性，而且要求所有生产者的transactional.id的取值一定要是唯一的，同一时刻只能有一个transactional.id存在，其他的将会被关闭。 KafkaProducerProducerConfig.TRANSACTIONAL_ID_CONFIG：生产者事务ID配置，必须保证全局唯一。(否则相同的ID中只会有一个生效)ProducerConfig.BATCH_SIZE_CONFIG：批处理大小，同一个事务内最多可以处理的消息个数。默认是16384。ProducerConfig.LINGER_MS_CONFIG：最大等待时间间隔，如果时间到了数量没有达到最大允许的批处理数量，直接执行。 initTransactions()：初始化事务配置。 beginTransaction()：开启事务。 commitTransaction()：提交事务。 abortTransaction()：终止事务。 sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, String groupId)：提交消费者的偏移量。 KafkaConsumerConsumerConfig.ISOLATION_LEVEL_CONFIG：读的事务隔离级别，开启事务后一定要设置为read_committed，不然等于没开。 小结 为什么会发生丢消息，不可靠？ 当ack确认为1时：Leader在确认记录后突然故障，此时副本因子还没有同步时，消息丢失。当ack确认为0时：不等待确认生产者投递消息可能未送达服务器，也会消息丢失。 什么情况下会有重复数据产生？ 确认时响应如果丢失，会导致retries机制重试，然后写多份。（幂等写已解决，但默认没开） 没有重复数据却消费多次或没有消费？ 可能是offset提交出现问题","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Kafka/"}],"author":"熊猫菌"},{"title":"Kafka 简单搭建","slug":"Kafka 简单搭建","date":"2021-05-11T15:51:01.789Z","updated":"2021-05-12T14:39:30.236Z","comments":true,"path":"2021/05/11/Kafka 简单搭建/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/11/Kafka%20%E7%AE%80%E5%8D%95%E6%90%AD%E5%BB%BA/","excerpt":"","text":"Kafka基础之简单搭建环境准备 Linux JDK1.8+ Zookeeper 下载地址 Kafka 下载地址 单机Kafka搭建注意事项： 配置主机名和IP映射 关闭防火墙&amp;防火墙开机自启动 配置基础环境 安装JDK，并配置好环境变量，最好先确认是否有Java，可以把老版本的卸载。 rpm包安装：rpm -ivh jdk-8u291-linux-aarch64.rpmtar.gz包安装：tar -zxvf jdk-8u291-linux-aarch64.tar.gzJava的安装目录，假设是/usr/java/jdk-8u291这样。 环境变量可以在当前用户级别配置即.bashrc文件，也可以去全局配置/etc/profile下配置。这里以.bashrc为例：JAVA_HOME=/usr/java/jdk-8u291PATH=$PATH:$JAVA_HOME/binexport JAVA_HOMEexport PPATH 最后要让改的配置生效source .bashrc 修改配置下的主机名和IP映射 主机名在/etc/sysconfig/network下。修改主机名：HOSTNAME=CentOS主机名随便取，这里叫CentOS。IP映射在/etc/hosts下。修改IP映射：192.168.181.128 CentOS，跟windows一样的配法。 关闭防火墙和防火墙的自启。 先使用systemctl firewalld status可以查看防火墙状态。直接systemctl firewalld stop可以关闭防火墙。最后systemctl disabled firewalld可以关闭防火墙的开机自启。 这里如果不想关闭防火墙，还可以自己暴露端口出去，使用firewall-cmd命令。如果提升命令没有，需要先安装yum install -y firewalld firewalld-config开启端口：firewall-cmd --zone=public --add-port=9092/tcp --permanent，可以自己添加。移除端口：firewall-cmd --zone=public --remove-port=9092/tcp --permanent也可移除。重启防火墙生效：firewall-cmd --reload或者systemctl firewalld restart重启。 配置zookeeper。 下载的tar.gz包先解压：tar -zxvf zookeeper-3.6.3.tar.gzZookeeper的安装目录，假设是/usr/zookeeper-3.6.3这样。 zookeeper的模板配置文件：/usr/zookeeper-3.6.3/conf/zoo_sample.cfg复制一份模板配置文件在同目录下：zoo.cfg。修改zoo.cfg的选项：dataDir=/root/zkdata变更数据目录。使用./usr/zookeeper-3.6.3/bin/zkServer.sh可以启动zk了。例：./bin/zkServer.sh start conf/zoo.cfg启动zk。 配置Kafka 下载的tar.gz包先解压：tar -zxvf kafka_2.11-2.6.2.tgzkafka的安装目录，假设是/usr/kafka_2.11-2.6.2这样。 kafka的模板配置文件：/usr/kafka_2.11-2.6.2/conf/server.properties修改server.properties选项：listeners=PLAINTEXT://CentOS:9092socket连接地址，这里最好填主机名，不要IP。修改server.properties选项：log.dirs=/usr/kafka-logs消息日志存储地址。修改server.properties选项：zookeeper.connect=CentOS:2181zk连接配置。使用./usr/kafka_2.11-2.6.2/bin/kafka-server-start.sh可以启动kafka了。例：./bin/kafka-server-start.sh -daemon config/server.properties后台启动。 配置Topic使用Kafka的kafka-topics.sh脚本来执行topic，分区，副本因子等配置。 连接上Broker服务器：./bin/kafka-topics.sh --bootstrap-server CentOS:9092 --create --topic topic01 --partitions 2 --replication-factor 1，创建了一个topic为topic01的主题，该topic下配置了2个分区日志数，同时为每一个分区日志配置了一个副本，CentOS作为Broker服务器主分区的Leader。 消费者开启：./bin/kafka-console-consumer.sh --bootstrap-server CentOS:9092 --topic topic01 --group group1，创建了一个消费者实例，消费topic为topic01这个主题下的消息，位于group1这个消费者组下，连接Broker服务器CentOS，这个时候group1只有一个消费者实例，所以会同时消费2个分区的数据。 生产者开启：./bin/kafka-console-producer.sh --broker-list CentOS:9092 --topic topic01，创建了一个生产者，将要想topic为topic01这个主题下投递消息，并连接当前主题的Broker服务器CentOS。 集群Kafka搭建注意事项(包含以上)： 配置同步时钟：ntpdate cn.pool.ntp.org，ntpdate ntp[1-7].aliyun.com[]里选填一个值就行。 集群间配置 在hosts文件中配置上多个服务器的IP映射 192.168.181.128 CentOSA,192.168.181.129 CentOSB,192.168.181.130 CentOSC等几台都可，这里是3台。同理集群中的其他服务器也需要这么配置。 配置zookeeper的配置文件zoo.cfg 在zoo.cfg后新增：server.1=CentOSA:2888:3888，server.2=CentOSA:2888:3888，server.3=CentOSA:2888:3888等服务器的地址。同理*32181端口：对cline端提供服务2888端口：集群内机器通讯使用（Leader监听此端口）3888端口：选举leader使用 配置Kafka集群 修改server.properties选项：zookeeper.connect=CentOSA:2181,CentOSB:2181,CentOSC:2181zk服务器连接配置，都配置上。同理*3修改server.properties选项：broker.id=1，broker.id=1，broker.id=2几个Broker的ID需要唯一配置。 集群间配置topic和单机版差不多的使用方式。多多利用命令–help，啥都有。还有官方文档可以查阅。 小结试了一下搭建，主要理解一下大致的运作，试试就好。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Kafka/"},{"name":"实践过程","slug":"实践过程","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B/"}],"author":"熊猫菌"},{"title":"Kafka 配置文件","slug":"Kafka 配置文件","date":"2021-05-11T15:51:01.786Z","updated":"2021-05-12T14:39:29.198Z","comments":true,"path":"2021/05/11/Kafka 配置文件/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/11/Kafka%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"","text":"Kafka基础之配置文件Kafka配置Kafka的主目录下 -&gt; conf目录下 -&gt; server.properties。主要常用的配置一般是broker.id，log.dirs，zookeeper.connect。 Kafka系统配置每一个broker在集群中的唯一标示，要求是正数。在改变IP地址，不改变broker.id的前提下，是不会影响consumers的消费的。broker.id = 0 Kafka的日志分区文件存放地址，核心的数据存储，多个地址的话采用逗号分割 /usr/kafka-logs-1，/usr/kafka-logs-2log.dirs = /tmp/kafka-logs 提供给客户端响应的端口port = 6667 消息提的最大大小，单位是字节message.max.bytes = 1000000 broker处理消息的最大线程数，一般情况下不需要去修改num.network.threads = 3 broker处理磁盘IO的线程数，数值应该大于你的硬盘数num.io.threads = 8 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改background.threads = 4 等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，算是一种自我保护机制queued.max.requests = 500 broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置host.name 打广告的地址，若是设置的话，会提供给producers, consumers,其他broker连接，具体如何使用还未深究advertised.host.name 广告地址端口，必须不同于port中的设置advertised.port socket的发送缓冲区，socket的调优参数SO_SNDBUFFsocket.send.buffer.bytes = 100 * 1024 socket的接受缓冲区，socket的调优参数SO_RCVBUFFsocket.receive.buffer.bytes = 100 * 1024 socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖socket.request.max.bytes = 100 * 1024 * 1024 Kafka日志配置topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖log.segment.bytes = 1024 * 1024 * 1024 这个参数会在日志segment没有达到log.segment.bytes设置的大小，也会强制新建一个segment 会被 topic创建时的指定参数覆盖log.roll.hours = 24 * 7 日志清理策略 选择有：delete和compact 主要针对过期数据的处理，或是日志文件达到限制的额度，会被 topic创建时的指定参数覆盖log.cleanup.policy = delete 数据存储的最大时间 超过这个时间 会根据log.cleanup.policy设置的策略处理数据，也就是消费端能够多久去消费数据，log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖log.retention.minutes = 7 days 指定日志每隔多久检查看是否可以被删除，默认1分钟log.cleanup.interval.mins = 1 topic每个分区的最大文件大小，一个topic的大小限制 = 分区数 * log.retention.bytes。-1没有大小限制，log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖log.retention.bytes = -1 文件大小检查的周期时间，是否处罚 log.cleanup.policy中设置的策略log.retention.check.interval.ms = 5 minutes 是否开启日志压缩log.cleaner.enable = false 日志压缩运行的线程数log.cleaner.threads = 1 日志压缩时候处理的最大大小log.cleaner.io.max.bytes.per.second = None 日志压缩去重时候的缓存空间 ，在空间允许的情况下，越大越好log.cleaner.dedupe.buffer.size = 500 * 1024 * 1024 日志清理时候用到的IO块大小 一般不需要修改log.cleaner.io.buffer.size = 512 * 1024 日志清理中hash表的扩大因子 一般不需要修改log.cleaner.io.buffer.load.factor = 0.9 检查是否处罚日志清理的间隔log.cleaner.backoff.ms = 15000 日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被topic创建时的指定参数覆盖log.cleaner.min.cleanable.ratio = 0.5 对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被topic创建时的指定参数覆盖log.cleaner.delete.retention.ms = 1 day 对于segment日志的索引文件大小限制，会被topic创建时的指定参数覆盖log.index.size.max.bytes = 10 * 1024 * 1024 当执行一个fetch操作后，需要一定的空间来扫描最近的offset大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数log.index.interval.bytes = 4096 log文件 “sync” 到磁盘之前累积的消息条数，因为磁盘IO操作是一个慢操作，但又是一个 “数据可靠性” 的必要手段，所以此参数的设置,需要在 “数据可靠性” 与 “性能” 之间做必要的权衡，如果此值过大,将会导致每次 “fsync” 的时间较长(IO阻塞)，如果此值过小,将会导致 “fsync” 的次数较多,这也意味着整体的client请求有一定的延迟，物理server故障,将会导致没有fsync的消息丢失。log.flush.interval.messages = None 检查是否需要固化到硬盘的时间间隔log.flush.scheduler.interval.ms = 3000 仅仅通过interval来控制消息的磁盘写入时机,是不足的，此参数用于控制 “fsync” 的时间间隔,如果消息量始终没有达到阀值,但是离上一次磁盘同步的时间间隔达到阀值,也将触发.log.flush.interval.ms = None 文件在索引中清除后保留的时间 一般不需要去修改log.delete.delay.ms = 60000 控制上次固化硬盘的时间点，以便于数据恢复 一般不需要去修改log.flush.offset.checkpoint.interval.ms = 60000 Kafka主题配置是否允许自动创建topic ，若是 false ，就需要通过命令创建topicauto.create.topics.enable = true 一个topic ，默认分区的replication个数 ，不得大于集群中broker的个数default.replication.factor = 1 每个topic的分区个数，若是在topic创建时候没有指定的话 会被topic创建时的指定参数覆盖num.partitions = 1 实例 –replication-factor 3 –partitions 1 –topic replicated-topic ：名称replicated-topic有一个分区，分区被复制到三个broker上。 Kafka副本配置partition leader与replicas之间通讯时,socket的超时时间controller.socket.timeout.ms = 30000 partition leader与replicas数据同步时,消息的队列尺寸controller.message.queue.size = 10 replicas响应partition leader的最长等待时间，若是超过这个时间，就将replicas列入ISR(in-sync replicas)，并认为它是死的，不会再加入管理中replica.lag.time.max.ms = 10000 如果follower落后与leader太多,将会认为此follower[或者说partition relicas]已经失效，通常,在follower与leader通讯时,因为网络延迟或者链接断开,总会导致replicas中消息同步滞后，如果消息之后太多,leader将认为此follower网络延迟较大或者消息吞吐能力有限,将会把此replicas迁移到其他follower中，在broker数量较少,或者网络不足的环境中,建议提高此值.replica.lag.max.messages = 4000 follower与leader之间的socket超时时间replica.socket.timeout.ms = 30 * 1000 leader复制时候的socket缓存大小replica.socket.receive.buffer.bytes = 64 * 1024 replicas每次获取数据的最大大小replica.fetch.max.bytes = 1024 * 1024 replicas同leader之间通信的最大等待时间，失败了会重试replica.fetch.wait.max.ms = 500 fetch的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会阻塞,直到满足条件replica.fetch.min.bytes = 1 leader 进行复制的线程数，增大这个数值会增加follower的IOnum.replica.fetchers = 1 每个replica检查是否将最高水位进行固化的频率replica.high.watermark.checkpoint.interval.ms = 5000 是否允许控制器关闭broker ,若是设置为 true ,会关闭所有在这个broker上的leader，并转移到其他brokercontrolled.shutdown.enable = false 控制器关闭的尝试次数controlled.shutdown.max.retries = 3 每次关闭尝试的时间间隔controlled.shutdown.retry.backoff.ms = 5000 是否自动平衡broker之间的分配策略auto.leader.rebalance.enable = false leader的不平衡比例，若是超过这个数值，会对分区进行重新的平衡leader.imbalance.per.broker.percentage = 10 检查leader是否不平衡的时间间隔leader.imbalance.check.interval.seconds = 300 客户端保留offset信息的最大空间大小offset.metadata.max.bytes Kafka ZK配置zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3zookeeper.connect = localhost:2181 ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大zookeeper.session.timeout.ms = 6000 ZooKeeper的连接超时时间zookeeper.connection.timeout.ms = 6000 ZooKeeper集群中leader和follower之间的同步实际那zookeeper.sync.time.ms = 2000 配置的修改其中一部分配置是可以被每个topic自身的配置所代替，例如 新增配置：bin/kafka-topics.sh –zookeeper localhost: 2181 –create –topic my-topic –partitions 1 –replication-factor 1 –config max.message.bytes= 64000 –config flush.messages= 1 修改配置：bin/kafka-topics.sh –zookeeper localhost: 2181 –alter –topic my-topic –config max.message.bytes= 128000 删除配置：bin/kafka-topics.sh –zookeeper localhost: 2181 –alter –topic my-topic –deleteConfig max.message.bytes CONSUMER 配置最为核心的配置是group.id、zookeeper.connect Consumer归属的组ID，broker是根据group.id来判断是队列模式还是发布订阅模式，非常重要group.id 消费者的ID，若是没有设置的话，会自增consumer.id 一个用于跟踪调查的ID ，最好同group.id相同client.id = group id value 对于zookeeper集群的指定，可以是多个 hostname1:port1,hostname2:port2,hostname3:port3 必须和broker使用同样的zk配置zookeeper.connect=localhost: 2182 zookeeper的心跳超时时间，查过这个时间就认为是dead消费者zookeeper.session.timeout.ms = 6000 zookeeper的等待连接时间zookeeper.connection.timeout.ms = 6000 zookeeper的follower同leader的同步时间zookeeper.sync.time.ms = 2000 当zookeeper中没有初始的offset时候的处理方式 。smallest ：重置为最小值largest:重置为最大值anything else：抛出异常auto.offset.reset = largest socket的超时时间，实际的超时时间是：max.fetch.wait + socket.timeout.ms.socket.timeout.ms = 30 * 1000 socket的接受缓存空间大小socket.receive.buffer.bytes = 64 * 1024 从每个分区获取的消息大小限制fetch.message.max.bytes = 1024 * 1024 是否在消费消息后将offset同步到zookeeper，当Consumer失败后就能从zookeeper获取最新的offsetauto.commit.enable = true 自动提交的时间间隔auto.commit.interval.ms = 60 * 1000 用来处理消费消息的块，每个块可以等同于fetch.message.max.bytes中数值queued.max.message.chunks = 10 当有新的consumer加入到group时,将会reblance,此后将会有partitions的消费端迁移到新的consumer上,如果一个consumer获得了某个partition的消费权限,那么它将会向zk注册”Partition Owner registry” 节点信息,但是有可能此时旧的consumer尚没有释放此节点,此值用于控制,注册节点的重试次数.rebalance.max.retries = 4 每次再平衡的时间间隔rebalance.backoff.ms = 2000 每次重新选举leader的时间refresh.leader.backoff.ms server发送到消费端的最小数据，若是不满足这个数值则会等待，知道满足数值要求fetch.min.bytes = 1 若是不满足最小大小(fetch.min.bytes)的话，等待消费端请求的最长等待时间fetch.wait.max.ms = 100 指定时间内没有消息到达就抛出异常，一般不需要改consumer.timeout.ms = -1 PRODUCER的配置比较核心的配置：metadata.broker.list、request.required.acks、producer.type、serializer.class 消费者获取消息元信息(topics, partitions and replicas)的地址,配置格式是：host1:port1,host2:port2，也可以在外面设置一个vipmetadata.broker.list 消息的确认模式0 ：不保证消息的到达确认，只管发送，低延迟但是会出现消息的丢失，在某个server失败的情况下，有点像TCP1 ：发送消息，并会等待leader 收到确认后，一定的可靠性-1 ：发送消息，等待leader收到确认，并进行复制操作后，才返回，最高的可靠性request.required.acks = 0 消息发送的最长等待时间request.timeout.ms = 10000 socket的缓存大小send.buffer.bytes = 100 * 1024 key的序列化方式，若是没有设置，同serializer.classkey.serializer.class 分区的策略，默认是取模partitioner.class = kafka.producer.DefaultPartitioner 消息的压缩模式，默认是none，可以有gzip和snappycompression.codec = none 可以针对默写特定的topic进行压缩compressed.topics= null 消息发送失败后的重试次数message.send.max.retries = 3 每次失败后的间隔时间retry.backoff.ms = 100 生产者定时更新topic元信息的时间间隔 ，若是设置为 0 ，那么会在每个消息发送后都去更新数据topic.metadata.refresh.interval.ms = 600 * 1000 用户随意指定，但是不能重复，主要用于跟踪记录消息client.id = &quot;&quot; Kafka消息模式配置生产者的类型 async:异步执行消息的发送 sync：同步执行消息的发送producer.type = sync 异步模式下，那么就会在设置的时间缓存消息，并一次性发送queue.buffering.max.ms = 5000 异步的模式下 最长等待的消息数queue.buffering.max.messages = 10000 异步模式下，进入队列的等待时间 若是设置为 0 ，那么要么进入队列，要么直接抛弃queue.enqueue.timeout.ms = -1 异步模式下，每次发送的最大消息数，前提是触发了queue.buffering.max.messages或是queue.buffering.max.ms的限制batch.num.messages = 200 消息体的系列化处理类 ，转化为字节流进行传输serializer. class = kafka.serializer.DefaultEncoder 小结配置是真的多，不过大多都用不上吧。 转自：kafka 配置文件参数详解","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Kafka/"},{"name":"配置文件","slug":"配置文件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"}],"author":"熊猫菌"},{"title":"Kafka 入门知识","slug":"Kafka 入门知识","date":"2021-05-10T16:25:52.596Z","updated":"2021-05-12T14:39:28.151Z","comments":true,"path":"2021/05/11/Kafka 入门知识/","link":"","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/2021/05/11/Kafka%20%E5%85%A5%E9%97%A8%E7%9F%A5%E8%AF%86/","excerpt":"","text":"Kafka基础之入门知识Kafka介绍Kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统，他可以收集并处理用户在网站中的所有动作流数据以及物流网设备的采样信息。 Kafka使用场景 系统间消息解耦 异步通信 削峰填谷 Kafka Streaming实时在线流处理 Kafka基础架构Kafka基本是以集群的形式存在的，以Topic形式负责分类集群中的Record，每一个Record属于一个Topic。每个Topic底层都会对应一组分区的日志用于持久化Topic中的Record。同时在Kafka集群中，Topic的每一个日志的分区都一定会有一个Broker担当该分区的Leader，其他的Broker担当该分区的Follower，Leader负责分区数据的读写操作，Follower负责同步该分区的数据。这样如果分区的Leader宕机，该分区的其他Follower会选取出新的Leader继续负责该分区数据的读写。其中集群中的Leader的监控和Topic的部分元数据是存储在zookeeper中。 简单场景示意 生产者：投递消息到一个topic中 消费者：可以同时订阅多个topic获取消息 消息分发策略 hash(key)%分区：根据key的hash值，将消息均匀的散列在同一个topic的不同分区上 分区：一个topic下可以配置多个分区 副本因子：每个主要分区备份数据的副本数量 Broker：一个Broker一定是至少主要负责某一个分区的读写，可以负责其他分区的副本。当某一个分区的Broker宕机后，zookeeper会重新选举一个已宕机的分区Leader出来，一个Broker可能身兼多个分区Leader。 Kafka分区日志Kafka中所有消息是通过Topic为单位进行管理，每个Kafka中的Topic通常会有多个订阅者，负责订阅发送到该Topic中的数据。Kafka负责管理集群中每个Topic的一组日志分区数据。 每组日志分区是一个有序的不可变的日志序列，分区中的每一个Record都被分配了唯一的序列编号称为是offset，Kafka集群会持久化所有发布到Topic中的Record消息，该Record的持久化时间是通过配置文件指定，默认是168小时。 1log.retention.hours&#x3D;168 Kafka底层会定期的检查日志文件，然后将过期的数据从log中移除，由于Kafka使用硬盘存储文件，因此使用Kafka长时间缓存一些日志文件是不存在问题的。 分区日志示意 分发分区策略：可以选择轮询或者hash等不同策略 old -&gt; new：消息有序按时间顺序增长，但是整个Topic内的顺序不能保证先进先出，只能保证单个分区是有序的。如果想作为先进先出的队列使用，建议不分区。 不能保证FIFO为啥还要对日志分区： 首先，它们允许日志扩展到超出单个服务器所能容纳的大小。每个单独的分区都必须适合托管它的服务器，但是一个Topic可能有很多分区，因此它可以处理任意数量的数据。 其次每个服务器充当某些分区的Leader，也可能充当其他分区的Follower，因此集群中的负载的得到了很好的平衡。 单个Topic的写入性能得到了极大的提升，不同的分区是由不同的Broker来负责读写，提升了吞吐量。 Kafka生产者&amp;消费者组消费者在消费Topic中的数据的时候，每个消费者会维护本次消费对应分区的偏移量(offset)，消费者会在消费玩一个批次的数据之后，会将本次消费的偏移量提交给Kafka集群，因此对于每个消费者而言可以随意的控制该消费者的偏移量。因此Kafka中，消费者可以从一个topic分区中的任意位置读取队列数据，由于每个消费者控制了自己的消费的偏移量，因此多个消费者之间彼此相互独立。 生产者&amp;消费者组 生产者偏移量：只管往后写，最后一个消息偏移量就是当前分区已写入的总消息量 消费者偏移量：消费者可以从分区的任意一个偏移量开始读，每次读之后，消费者会主动通知Kafka当前已读的偏移量，值是下一个偏移量。即当访问了偏移量为15时，提交访问偏移量为16。 消费者组：消费者会使用Consumer Group名称来标识自己，并且发布到Topic的每条记录都会传递到每个订阅Consumer Group中的一个消费者实例，如果所有的消费者实例都具有相同的Consumer Group，那么Topic中的记录会在该Consumer Group中Consumer实例进行均分消费；如果所有的Consumer实例具有不同的Consumer Group，则每条记录会广播到所有的Consumer Group进程。 简而言之：一个Consumer Group可以理解为一个逻辑上的订阅者。它由多个Consumer实例组成，以实现可伸缩性和容错性能力。Topic按照分区的方式均分给一个Consumer Group下的所有实例，如果Consumer Group有新成员加入，则它会分担其他消费者负责的某些分区；同理如果一个Consumer Group下有实例宕机，则由该Group下的其他实例接管宕机的实例所负责的分区。 当消费者组内的消费者实例数大于Topic分区数时：多于的消费者实例会闲着，当存在已被分区的实例宕机时，会自动接管宕机实例的分区进行消费。 Kafka顺序写入和mmapKafka的特性之一就是高吞吐量，但是Kafka的消息是保存或缓存在磁盘上的，一般认为在磁盘上的读写数据是会降低性能的，但是Kafka即使是普通的服务器也可以轻松支持秒级百万的写入请求，超过了大部分的消息中间件，这种特性也使得Kafka在日志处理等海量数据场景应用广泛。Kafka会把收到的数据都写入到磁盘上，为了防止丢数据，优化写入速度，Kafka采用了2个技术：顺序写入和MMFile。 顺序写入：硬盘是机械结构，每次读写都会1.寻址 -&gt; 2.写入。其中寻址是一个最耗时的动作，所以硬盘最讨厌随机IO，喜欢顺序IO，所以为了提高硬盘的读写速度，Kafka就是使用的顺序IO。这样省去了大量的内存开销以及节省了IO寻址的时间。但是单纯的使用顺序写入，Kafka的写入性能也不可能和内存进行对比，因此Kafka的数据并不是实时的写入磁盘中。 MMFile：Kafka充分利用了现代操作系统分页存储来利用内存提高IO效率。Memory Mapped Files(mmap)内存映射文件，在64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page实现文件到物理内存的直接映射。完成mmap映射后，用户对内存的所有操作会被操作系统自动的刷新到磁盘上，极大地降低了IO使用率。 顺序写&amp;MMF 用户空间：应用一般都是运行在用户空间下，只需要将数据写入到内存页PageCache中即可，后面不需要等待缓存刷新到磁盘的过程，而且即使应用宕机，也并不会影响已经写入内存页的数据丢失。 内核空间：由操作系统底层自己控制，自动将PageCache上的数据刷到磁盘上，没有用户空间切换下，减少了一定的IO，相对的，可支持的IO就更大。 问题：如果内核不稳定，出现问题，就会导致应用没有故障还是丢失数据的问题。毕竟高吞吐量和一致性不能全部都万无一失。 Kafka读取零拷贝Kafka客户端在响应客户端读取的时候，底层使用Zero Copy(零拷贝)技术，直接将磁盘无需拷贝到用户空间，而是直接将数据通过内核空间传递输出，数据并没有抵达用户空间。 传统IO操作示意图 传统IO操作流程： 用户进程调用read等系统调用向操作系统发出IO请求，请求读取数据到自己的内存缓冲区中，自己进入阻塞状态。 操作系统收到请求后，进一步将IO请求发送磁盘。 磁盘驱动器收到内核的IO请求，把数据从磁盘读取到驱动器的缓冲中，此时不占用CPU。当驱动器的缓冲区被读满后，向内核发起中断信号告知自己缓冲区已满。 内核收到中断，使用CPU时间将磁盘驱动器中缓冲中的数据拷贝到内核缓冲区中。 如果内核缓冲区的数据少于用户申请的读的数据，重复步骤3和步骤4，直到内核缓冲区的数据足够多为止。 将数据从内核缓冲区拷贝到用户缓冲区，同时从系统调用中返回，回到用户空间，完成任务。 DMA示意图 DMA：协处理器，协助CPU做IO调度。 相对于传统IO：减少了CPU控制中断的次数，不妨碍CPU的执行计算，可以大大提高CPU的计算能力。 传统或DMA模式下IO 用户访问服务器正常读取流程： 文件在磁盘中数据被copy到内核缓冲区。 从内核缓冲区copy到用户缓冲区。 用户缓冲区copy到内核与socket相关的缓冲区。 数据从socket缓冲区copy到相关协议引擎发送出去。 一共经历了4次数据拷贝，2次用户态和内核态的切换。 零拷贝示意图 零拷贝下的读取流程： 文件在磁盘中数据被copy到内核缓冲区。 从内核缓冲区copy到内核与socket相关的缓冲区。 数据从socket缓冲区copy到相关协议引擎发送出去。 一共经历了3次数据拷贝，没有用户态和内核态的切换。 小结Kafka为什么读入和写入性能高？ 分区特性决定了读入和写入的性能，重点在高吞吐量。 顺序写入和MMF决定了写入性能的提升。 零拷贝决定了读取性能的提升。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Kafka/"},{"name":"大数据","slug":"大数据","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"消息队列","slug":"消息队列","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"author":"熊猫菌"}],"categories":[{"name":"缓存中间件","slug":"缓存中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E7%BC%93%E5%AD%98%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"负载调度器","slug":"负载调度器","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E8%B4%9F%E8%BD%BD%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Redis/"},{"name":"缓存","slug":"缓存","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E7%BC%93%E5%AD%98/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"代理","slug":"代理","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E4%BB%A3%E7%90%86/"},{"name":"Nginx","slug":"Nginx","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Nginx/"},{"name":"实践过程","slug":"实践过程","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B/"},{"name":"LVS","slug":"LVS","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/LVS/"},{"name":"Kafka","slug":"Kafka","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/Kafka/"},{"name":"流处理","slug":"流处理","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E6%B5%81%E5%A4%84%E7%90%86/"},{"name":"配置文件","slug":"配置文件","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"大数据","slug":"大数据","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"消息队列","slug":"消息队列","permalink":"https://github.com/moexiong/moexiong.github.io/tree/master/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]}